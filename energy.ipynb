{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:01:49.822571Z",
     "start_time": "2020-03-17T20:01:45.810273Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matti\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import operator\n",
    "\n",
    "import linear_model_tests as lmt\n",
    "import decision_tree_opt as dto\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "lin_mod = lmt.Test()\n",
    "opt_tree = dto.Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:01:54.185275Z",
     "start_time": "2020-03-17T20:01:53.640062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19735, 25)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"energydata_complete.csv\")\n",
    "data = data.dropna()\n",
    "names = [\"date\",\"appliance_wh\", \"light_wh\",\"kitchen_celsius\",\"kitchen_hum_perc\",\n",
    "        \"living_celsius\",\"living_hum_perc\",\"laundry_celsius\",\"laundry_hum_perc\",\"office_celsius\",\"office_hum_perc\",\n",
    "         \"bathroom_celsius\",\"bathroom_hum_perc\",\"portico_celsius\",\"portico_hum_perc\",\"ironing_celsius\",\"ironing_hum_perc\",\n",
    "         \"teen_celsius\",\"teen_hum_perc\",\"parents_celsius\",\"parents_hum_perc\",\"cws_celsius\",\"cws_pressure\",\"cws_hum_perc\",\n",
    "         \"cws_wind\",\"cws_visibility\",\"cws_dew_point\",\"rv1\",\"rv2\" ]\n",
    "\n",
    "data = data.rename(columns = dict(zip(data.columns, names)))\n",
    "data = data.drop([\"rv1\",\"rv2\",\"date\"], axis = 1)\n",
    "data = data.drop_duplicates(data.columns, keep = \"last\")\n",
    "#data = data.drop(data[(data.appliance_wh > 790)|(data.appliance_wh < 0)].index)\n",
    "data = data.reset_index()\n",
    "data = data.drop([\"index\", \"light_wh\"], axis = 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:47:35.396092Z",
     "start_time": "2020-03-17T15:47:29.963989Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,8))\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:27:09.763142Z",
     "start_time": "2020-03-17T20:27:09.695427Z"
    }
   },
   "outputs": [],
   "source": [
    "def MSE(true,pred):\n",
    "    a = round(mean_squared_error(y_true = true, y_pred = pred), 5)\n",
    "    return a/len(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:02:04.933505Z",
     "start_time": "2020-03-17T20:02:04.807628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19735, 1) (19735, 24)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(\"appliance_wh\", axis = 1).values\n",
    "y = data.appliance_wh\n",
    "X = X.reshape( (len(X), len(X[0])))\n",
    "y = y.values.reshape( len(y), 1)\n",
    "print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:02:07.068660Z",
     "start_time": "2020-03-17T20:02:06.956366Z"
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(len(X)*0.66)\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_test  = X[train_size:]\n",
    "y_test  = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:28:22.025916Z",
     "start_time": "2020-03-17T20:28:21.941143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.74367193243762\n",
      "Test Error: 1.2450432131147542\n",
      "\n",
      "R^2 =  0.12153\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "model = LinearRegression(fit_intercept = True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print (\"Train Error:\", MSE(y_train, y_pred[:train_size]))\n",
    "print (\"Test Error:\",  MSE(y_test, y_pred[train_size:]))\n",
    "print()\n",
    "print(\"R^2 = \", round(r2_score(y, y_pred), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:11:19.345775Z",
     "start_time": "2020-03-17T20:11:12.545113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing features with LASSO \n",
      "\t ## Remaining features: 15 \n",
      "\t ## R^2 after reduction: 0.19733\n"
     ]
    }
   ],
   "source": [
    "lasso = lin_mod.lasso(data, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:02:25.620319Z",
     "start_time": "2020-03-17T20:02:23.575673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing features with Recursive Feature Elimination \n",
      "\t ## Remaining features: 14 \n",
      "\t ## R^2 after reduction: 0.18978\n"
     ]
    }
   ],
   "source": [
    "recursive = lin_mod.recurs_elimin(data, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POLYNOMIAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLINE: https://www.analyticsvidhya.com/blog/2018/03/introduction-regression-splines-python-codes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:30:39.593401Z",
     "start_time": "2020-03-17T20:30:24.204702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.5842003163147793\n",
      "Test Error: 3.84992776900149\n",
      "R^2 =  -0.31345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree = 2, include_bias = False, interaction_only = True)\n",
    "features_polynomial = poly.fit_transform(X_train)\n",
    "\n",
    "model_2 = LinearRegression(fit_intercept = True)\n",
    "model_2.fit(features_polynomial, y_train)\n",
    "\n",
    "y_pred_2 = model_2.predict( poly.transform(X) )\n",
    "\n",
    "print (\"Train Error:\", MSE(y_train, y_pred_2[:train_size]) )\n",
    "print (\"Test Error:\",  MSE(y_test, y_pred_2[train_size:]) )\n",
    "print(\"R^2 = \", round(r2_score(y, y_pred_2), 5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:53:05.498605Z",
     "start_time": "2020-03-17T20:53:05.367928Z"
    }
   },
   "outputs": [],
   "source": [
    "def bias_var(X, y):\n",
    "    t_size = int(len(X)*0.66)\n",
    "    X_train = X[:t_size]\n",
    "    y_train = y[:t_size]\n",
    "    X_test = X[t_size:]\n",
    "    y_test = y[t_size:]\n",
    "    \n",
    "    preds = []\n",
    "    # The loop is iterated over a couple of equivalent parameters\n",
    "    # to reduce the execution time, at the expense of reduction of results \n",
    "    for leave, samp in enumerate(range(2, 101)):\n",
    "        dt = DecisionTreeClassifier(max_leaf_nodes = leave+2, min_samples_split = samp)\n",
    "        model = dt.fit(X, y)\n",
    "        preds += [ list(model.predict(X)) ] # every element is y_pred\n",
    "        \n",
    "    stats = []    \n",
    "    for x in range(len(preds)):                           \n",
    "        dt_variance = np.var(preds[x])\n",
    "        dt_bias = MSE(y, preds[x]) - dt_variance  # bias^2 = MSE - variance    # (y - np.mean(preds[x]))**2\n",
    "        acc = accuracy_score(y_true = y, y_pred = preds[x])\n",
    "        stats += [ (dt_bias.mean(), dt_variance, round(acc, 6)) ]\n",
    "        \n",
    "    stats = [ list(x) for x in stats ]\n",
    "    df = pd.DataFrame(stats, columns = [\"bias\", \"variance\", \"accuracy\"])\n",
    "    df[\"bias_plus_var\"] = df.bias + df.variance\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T20:53:08.927910Z",
     "start_time": "2020-03-17T20:53:08.802477Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-53d61d1470c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstats_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbias_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_tree' is not defined"
     ]
    }
   ],
   "source": [
    "stats_df = bias_var(X_tree, y_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:54.838359Z",
     "start_time": "2020-03-17T14:29:54.696653Z"
    }
   },
   "outputs": [],
   "source": [
    "print(stats_df.loc[stats_df.bias_plus_var == min(stats_df.bias_plus_var)])\n",
    "print()\n",
    "print(stats_df.loc[stats_df.accuracy == max(stats_df.accuracy)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T11:10:03.579006Z",
     "start_time": "2020-03-17T10:28:23.566055Z"
    }
   },
   "source": [
    "To assess the parameter of the Tree Classifier, we performed a simulation to see how to set the ```max_leaf_nodes``` parameter in such a way that the the sum of bias$^2$ and Variance is minimized. This led to the value of ```39```, but we have seen a decrease in the accuracy of the model from 0.69 to 0.26.\n",
    "<p>\n",
    "*****The assement on ensemble method that is more suitable for this instance was made on the bias and variance value of the actual model, without the simulation implemented: since the variance is far low than the bias. <p>\n",
    "\n",
    "```python\n",
    "stats_df = opt_tree.bias_var(X_tree, y_tree) # long to be executed\n",
    "print(stats_df.loc[stats_df.bias_plus_var == min(stats_df.bias_plus_var)])\n",
    "print()\n",
    "print(stats_df.loc[stats_df.accuracy == max(stats_df.accuracy)])\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax1 = plt.subplots(figsize = (9, 5))\n",
    "\n",
    "ax1.set_xlabel('Model Complexity')\n",
    "ax1.set_ylabel('Bias$^2$ + Variance', color = \"#3DA5D9\")\n",
    "ax1.plot(range(len(stats_df)), stats_df.bias_plus_var, color = \"#3DA5D9\", linewidth = 1, alpha = 0.7)\n",
    "ax1.plot(range(len(stats_df)), stats_df.variance, color = \"#DB504A\", linewidth = 1, alpha = 0.7)\n",
    "\n",
    "ax1.tick_params(axis = 'y')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Accuracy', color = \"#EA7317\")\n",
    "ax2.plot(range(len(stats_df)), stats_df.accuracy, color = \"#EA7317\", linewidth = 1, alpha = 0.7)\n",
    "ax2.tick_params(axis = 'y')\n",
    "fig.tight_layout()\n",
    "plt.savefig('Bias_variance_accuracy.png', dpi = 300)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T21:25:56.633617Z",
     "start_time": "2020-03-17T21:25:56.362375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.24044\n",
      "\n",
      "Bias^2: -342.4279 --- Variance: 343.0025\n",
      "MSE: 0.574532494552825\n"
     ]
    }
   ],
   "source": [
    "X_tree = data.drop(\"appliance_wh\", axis = 1).values\n",
    "y_tree = data.appliance_wh\n",
    "X_tree = X_tree.reshape( (len(X_tree), len(X_tree[0])))\n",
    "y_tree = y_tree.values.reshape( len(y_tree), 1)\n",
    "\n",
    "X_train_tree = X_tree[:train_size]\n",
    "y_train_tree = y_tree[:train_size]\n",
    "X_test_tree  = X_tree[train_size:]\n",
    "y_test_tree  = y_tree[train_size:]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "decisiontree = DecisionTreeClassifier(max_leaf_nodes = 39, min_samples_split = 39)\n",
    "model_tree = decisiontree.fit(X_train_tree, y_train_tree)\n",
    "\n",
    "acc = accuracy_score(y_true = y_tree, y_pred = model_tree.predict(X_tree))\n",
    "print(\"Model Accuracy:\", round(acc, 5))\n",
    "print()\n",
    "tree_pred = model_tree.predict(X_tree)\n",
    "\n",
    "tree_variance = np.var(tree_pred)\n",
    "tree_bias = MSE(y_tree, tree_pred) - tree_variance\n",
    "tree_error = MSE(y_tree, tree_pred)\n",
    "\n",
    "print(\"Bias^2:\", round(tree_bias.mean(), 4), \"--- Variance:\", round(tree_variance, 4) )\n",
    "print(\"MSE:\", tree_error)\n",
    "importance = dict(zip(data.drop(\"appliance_wh\", axis = 1).columns, model_tree.feature_importances_))\n",
    "importance = pd.DataFrame( sorted(importance.items(), key = operator.itemgetter(1))[::-1] ) # sort by value\n",
    "importance = importance.rename(columns = {0: \"col\", 1:\"val\"})\n",
    "#importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T21:20:41.137721Z",
     "start_time": "2020-03-17T21:20:41.068003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6071428571428572\n",
      "1.3720238095238095\n"
     ]
    }
   ],
   "source": [
    "def variance(l):\n",
    "    var = [ ((l[x] - ( sum(l)/len(l) ))**2) for x in range(len(l)) ]\n",
    "    return (sum(l)/len(l))\n",
    "\n",
    "print(variance([2.75, 1.75, 1.25, 0.25, 0.5, 1.25, 3.5]))\n",
    "print(statistics.variance([2.75, 1.75, 1.25, 0.25, 0.5, 1.25, 3.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T18:04:58.871602Z",
     "start_time": "2020-03-17T18:04:26.209202Z"
    }
   },
   "outputs": [],
   "source": [
    "#### BAGGING\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagged = BaggingClassifier(decisiontree)\n",
    "bagged_2 = BaggingClassifier(bagged)\n",
    "bagged_fit = bagged_2.fit(X_train_tree, y_train_tree)\n",
    "\n",
    "\n",
    "print (\"Accuracy:\", accuracy_score(y_true = y_tree, y_pred = bagged_fit.predict(X_tree)) )\n",
    "\n",
    "bagg_bias = (y_tree - np.mean(bagged_fit.predict(X_tree)))**2\n",
    "bagg_variance = np.var(bagged_fit.predict(X_tree))\n",
    "print(\"Bias^2:\", round(bagg_bias.mean(), 4), \"--- Variance:\", round(bagg_variance, 4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-17T18:04:27.905Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dtime\n",
    "#### BOOSTING\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(bagged_2)\n",
    "print(dtime.datetime.now())\n",
    "adaboost_2 = AdaBoostClassifier(adaboost)\n",
    "print(dtime.datetime.now())\n",
    "adaboost_fit = adaboost_2.fit(X_train_tree, y_train_tree)\n",
    "print(dt.datetime.now())\n",
    "print (\"Accuracy:\", accuracy_score(y_true = y_tree, y_pred = adaboost_fit.predict(X_tree)) )\n",
    "ada_bias = (y_tree - np.mean(adaboost_fit.predict(X_tree)))**2\n",
    "ada_variance = np.var(adaboost_fit.predict(X_tree))\n",
    "print(\"Bias^2:\", round(ada_bias.mean(), 4), \"--- Variance:\", round(ada_variance, 4) )\n",
    "\n",
    "\n",
    "#### Stacking\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#estimators = [ (\"boos\", adaboost), (\"bagg\", bagged) ]\n",
    "\n",
    "#stack = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression())\n",
    "#stack_fit = stack.fit(X_train_tree, y_train_tree)\n",
    "\n",
    "#print (\"Accuracy:\", accuracy_score(y_true = y_tree, y_pred = stack_fit.predict(X_tree)) )\n",
    "#sta_bias = (y_tree - np.mean(stack_fit.predict(X_tree)))**2\n",
    "#sta_variance = np.var(stack_fit.predict(X_tree))\n",
    "#print(\"Bias^2:\", round(sta_bias.mean(), 4), \"--- Variance:\", round(sta_variance, 4) )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T15:27:59.488366Z",
     "start_time": "2020-03-16T15:27:53.982142Z"
    }
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import pydotplus\n",
    "import pydot\n",
    "from IPython.display import Image\n",
    "from graphviz import Graph\n",
    "from sklearn import tree\n",
    "\n",
    "dot_data = tree.export_graphviz(decisiontree, out_file = 'tree.dot',\n",
    "                                filled = True, rounded = True, special_characters = True)\n",
    "\n",
    "#(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "#graph = graphviz.Source(dot_data)\n",
    "#graph.write_png('tree.png')\n",
    "\n",
    "# Draw graph\n",
    "graph = pydot.graph_from_dot_data(dot_data)#.getvalue())\n",
    "# Show graph\n",
    "#Image(graph.create_png())\n",
    "graph.write_png(\"tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
